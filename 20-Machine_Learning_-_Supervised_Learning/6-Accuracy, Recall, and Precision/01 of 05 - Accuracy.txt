ACCURACY, RECALL, PRECISION, AND F1 SCORE
Accuracy
After creating a machine learning algorithm capable of making classifications, the next step in the process is to calculate its predictive power. In order to calculate these statistics, we’ll need to split our data into a training set and validation set.

Let’s say you’re using a machine learning algorithm to try to predict whether or not you will get above a B on a test. The features of your data could be something like:

The number of hours you studied this week.
The number of hours you watched Netflix this week.
The time you went to bed the night before the test.
Your average in the class before taking the test.
The simplest way of reporting the effectiveness of an algorithm is by calculating its accuracy. Accuracy is calculated by finding the total number of correctly classified points and dividing by the total number of points.

In other words, accuracy can be defined as:

(True Positives + True Negatives) / (True Positives + True Negatives + False Positives + False Negatives)

Let’s define those terms in the context of our grade example :

True Positive: The algorithm predicted you would get above a B, and you did.
True Negative: The algorithm predicted you would get below a B, and you did.
False Positive: The algorithm predicted you would get above a B, and you didn’t.
False Negative: The algorithm predicted you would get below a B, and you didn’t.
Let’s calculate the accuracy of a classification algorithm!

Instructions
1.
Create four variables that start at 0. They should be called true_positives, true_negatives, false_positives, and false_negatives

2.
We’ve given you two lists. labels represents the true labels of your dataset. Each 1 represents a test that you got above a B on, and each 0 represents a test that was below a B. guesses represents the classifications a machine learning algorithm might return. For every test, the classifier guessed whether your grade was above or below a B.

Loop through guesses and add 1 to true_positives every time your algorithm found a true positive. A true positive is when the real label and the classifier’s guess are both 1.


Hint
Your for loop should look like this:

for i in range(len(guesses)):
You should add one to true_positives if guesses[i] == 1 and labels[i] == 1.

3.
Inside the for loop, count the number of true negatives, false positives, and false negatives.


Hint
A true negative is when guesses[i] == 0 and labels[i] == 0.

A false positive is when guesses[i] == 1 and labels[i] == 0.

A false negative is when guesses[i] == 0 and labels[i] == 1.

4.
Outside of the for loop, calculate the accuracy and store it in a variable named accuracy. Print accuracy.


Hint
Accuracy is (True Positives + True Negatives) / (True Positives + True Negatives + False Positives + False Negatives).

Your denominator could also be len(guesses).