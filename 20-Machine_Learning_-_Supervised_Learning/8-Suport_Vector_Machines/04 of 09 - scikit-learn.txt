SUPPORT VECTOR MACHINES
scikit-learn
Now that we know the concepts behind SVMs we need to write the code that will find the decision boundary that maximizes the margin. All of the code that we’ve written so far has been guessing and checking — we don’t actually know if we’ve found the best line. Unfortunately, calculating the parameters of the best decision boundary is a fairly complex optimization problem. Luckily, Python’s scikit-learn library has implemented an SVM that will do this for us.

Note that while it is not important to understand how the optimal parameters are found, you should have a strong conceptual understanding of what the model is optimizing.

To use scikit-learn’s SVM we first need to create an SVC object. It is called an SVC because scikit-learn is calling the model a Support Vector Classifier rather than a Support Vector Machine.

classifier = SVC(kernel = 'linear')
We’ll soon go into what the kernel parameter is doing, but for now, let’s use a 'linear' kernel.

Next, the model needs to be trained on a list of data points and a list of labels associated with those data points. The labels are analogous to the color of the point — you can think of a 1 as a red point and a 0 as a blue point. The training is done using the .fit() method:

training_points = [[1, 2], [1, 5], [2, 2], [7, 5], [9, 4], [8, 2]]
labels = [1, 1, 1, 0, 0, 0]
classifier.fit(training_points, labels) 
The graph of this dataset would look like this:

An SVM with a decision boundary very close to the blue points.
Calling .fit() creates the line between the points.

Finally, the classifier predicts the label of new points using the .predict() method. The .predict() method takes a list of points you want to classify. Even if you only want to classify one point, make sure it is in a list:

print(classifier.predict([[3, 2]]))
In the image below, you can see the unclassified point [3, 2] as a black dot. It falls on the red side of the line, so the SVM would predict it is red.

An SVM with a decision boundary very close to the blue points.
In addition to using the SVM to make predictions, you can inspect some of its attributes. For example, if you can print classifier.support_vectors_ to see which points from the training set are the support vectors.

In this case, the support vectors look like this:

[[7, 5],
 [8, 2],
 [2, 2]]
Instructions
1.
Let’s start by making a SVC object with kernel = 'linear'. Name the object classifier.


Hint
classifier = SVC(kernel = 'linear')
2.
We’ve imported the training set and labels for you. Call classifier‘s .fit() method using points and labels as parameters.


Hint
classifier.fit(points, ____)
Fill in the second parameter in the code above.

3.
We can now classify new points. Try classifying both [3, 4] and [6, 7]. Remember, the .predict() function expects a list of points to predict.

Print the results.


Hint
Use [[3, 4], [6, 7]] as the parameter of .predict().